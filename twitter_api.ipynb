{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"./.env.local\")\n",
    "import os\n",
    "BEARER_TOKEN = os.getenv(\"BEARER_TOKEN\")\n",
    "import tweepy\n",
    "client = tweepy.Client(BEARER_TOKEN, wait_on_rate_limit=True)\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bao\\AppData\\Local\\Temp\\ipykernel_10852\\3492015089.py:6: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  full_dataset = pd.read_csv(day, compression='gzip')\n",
      "C:\\Users\\Bao\\AppData\\Local\\Temp\\ipykernel_10852\\3492015089.py:6: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  full_dataset = pd.read_csv(day, compression='gzip')\n",
      "C:\\Users\\Bao\\AppData\\Local\\Temp\\ipykernel_10852\\3492015089.py:6: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  full_dataset = pd.read_csv(day, compression='gzip')\n",
      "C:\\Users\\Bao\\AppData\\Local\\Temp\\ipykernel_10852\\3492015089.py:6: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  full_dataset = pd.read_csv(day, compression='gzip')\n",
      "C:\\Users\\Bao\\AppData\\Local\\Temp\\ipykernel_10852\\3492015089.py:6: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  full_dataset = pd.read_csv(day, compression='gzip')\n",
      "C:\\Users\\Bao\\AppData\\Local\\Temp\\ipykernel_10852\\3492015089.py:6: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  full_dataset = pd.read_csv(day, compression='gzip')\n",
      "C:\\Users\\Bao\\AppData\\Local\\Temp\\ipykernel_10852\\3492015089.py:6: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  full_dataset = pd.read_csv(day, compression='gzip')\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the datasets and save them in csv files under csvdataframes folder\n",
    "Path.mkdir(Path(\"csvdataframes\"), exist_ok=True)\n",
    "dataset_path = Path(\"dataset\")\n",
    "\n",
    "for idx, day in enumerate(dataset_path.iterdir()):\n",
    "    full_dataset = pd.read_csv(day, compression='gzip')\n",
    "    df_en = full_dataset[full_dataset['language']=='en']\n",
    "    df_en_filteted = df_en[[\"userid\", \"tweetid\", \"text\", \"hashtags\"]]\n",
    "    df_no_duplicate = df_en_filteted.drop_duplicates(subset='text', keep='first')\n",
    "    df_sampled = df_no_duplicate.sample(1)\n",
    "    df_sampled.to_csv(Path(f\"csvdataframes/day_{idx}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to call on each csv day files\n",
    "def create_edgelist(df_sample):  \n",
    "#     edgelist_retweet = open(f\"edgelists/edgelist_retweet_{time.time()}.txt\", \"a\")\n",
    "    edgelist_retweet = open(f\"edgelists/edgelist_retweet.txt\", \"a\")\n",
    "    edgelist_retweet.write(\"tweet_id rt_user_id\\n\")\n",
    "\n",
    "#     edgelist_like = open(f\"edgelists/edgelist_like_{time.time()}.txt\", \"a\")\n",
    "    edgelist_like = open(f\"edgelists/edgelist_likes.txt\", \"a\")\n",
    "    \n",
    "    edgelist_like.write(\"tweet_id like_user_id\\n\")\n",
    "\n",
    "    for index, row in df_sample.iterrows():\n",
    "        tweetid = row['tweetid']\n",
    "        print(f\"{tweetid=}\")\n",
    "        tweet = client.get_tweet(id=tweetid, expansions='referenced_tweets.id')\n",
    "        \n",
    "        # Deleted tweets are still in the dataset => just skip them for now\n",
    "        if (tweet.errors):\n",
    "            continue\n",
    "            \n",
    "        og_tweet_id = tweet.includes.get('tweets')[0].id if tweet.includes.get('tweets') else tweetid\n",
    "\n",
    "        # Get users who retweeted\n",
    "        users_retweet = client.get_retweeters(id=og_tweet_id)\n",
    "        \n",
    "        # Skip if no retweet\n",
    "        if not users_retweet.data:\n",
    "            continue\n",
    "            \n",
    "        for user in users_retweet.data:\n",
    "            edgelist_retweet.write(f\"{og_tweet_id} {user.id}\\n\")\n",
    "            continue\n",
    "\n",
    "        next_token = users_retweet.meta.get('next_token')\n",
    "\n",
    "        while(next_token):\n",
    "            users_retweet = client.get_retweeters(id=og_tweet_id, pagination_token=next_token)\n",
    "            \n",
    "            if not users_retweet.data:\n",
    "                continue\n",
    "            \n",
    "            for user in users_retweet.data:\n",
    "                edgelist_retweet.write(f\"{og_tweet_id} {user.id}\\n\")\n",
    "                \n",
    "            next_token = users_retweet.meta.get('next_token')\n",
    "\n",
    "        # Get users who liked\n",
    "        users_like = client.get_liking_users(id=og_tweet_id)\n",
    "        \n",
    "        # Skip if no like\n",
    "        if not users_like.data:\n",
    "            continue\n",
    "        \n",
    "        for user in users_like.data:\n",
    "            edgelist_like.write(f\"{og_tweet_id} {user.id}\\n\")\n",
    "            \n",
    "        next_token = users_like.meta.get('next_token')\n",
    "\n",
    "        while(next_token):\n",
    "            users_like = client.get_retweeters(id=og_tweet_id, pagination_token=next_token)\n",
    "            \n",
    "            if not users_like.data:\n",
    "                continue\n",
    "            \n",
    "            for user in users_like.data:\n",
    "                edgelist_like.write(f\"{og_tweet_id} {user.id}\\n\")\n",
    "                \n",
    "            next_token = users_like.meta.get('next_token')\n",
    "\n",
    "    edgelist_retweet.close()\n",
    "    edgelist_like.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweetid=1497766842580021248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 888 seconds.\n",
      "Rate limit exceeded. Sleeping for 888 seconds.\n",
      "Rate limit exceeded. Sleeping for 888 seconds.\n",
      "Rate limit exceeded. Sleeping for 888 seconds.\n",
      "Rate limit exceeded. Sleeping for 889 seconds.\n",
      "Rate limit exceeded. Sleeping for 889 seconds.\n",
      "Rate limit exceeded. Sleeping for 889 seconds.\n",
      "Rate limit exceeded. Sleeping for 889 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "Path.mkdir(Path(\"edgelists\"), exist_ok=True)\n",
    "for file in Path(\"csvdataframes\").iterdir():\n",
    "    df = pd.read_csv(file)\n",
    "    create_edgelist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ea647f73e73e5606616d713d17f7e03fc573d6527f7718c378c29b419b358a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
